---
layout: page
title: "Outline"
date: "2018"
description: "an outline"

---



# Getting Started

## What's Deep Learning

Talk talk

## What's it good for

Lots of examples. Weave some history in here.

## Kicking the tires

Example

## How does it work

High level overview


# Data

## What defines a good dataset

Your dataset should be as close as possible to your expected data.

### Representative
One thing that's important is to provide data that is representative of the data you wish to see. For instance, if your training data consists of well-lit data like:

![Categories of data](categories.svg)

yet the data you predict on looks like:

![Prediction](pred.svg)

the machine will struggle to predict successfully.

Commonly with image data, a non-representative dataset will appear when a model is trained only with well-lit, high quality photography, only to receive grainy, low resolution uploaded photos from users. In this case, the model often fails to correctly identify and classify photos. 

Remember, a neural net brings no common sense to the table. It only knows what it sees during training. It'd be like training your dog to "Sit!" on command, then switch from English to Spanish and expect her to understand.

### Well-Balanced
You also want your dataset to be well balanced. You want as close to the same number of data points per category. If one category is overrepresented, you have an **unbalanced dataset**. An unbalanced dataset can lead to problems with prediction. For instance, if 90% of your data belongs to category one, your model might very well learn to assume that everything belongs to category one. After all, 90% of the time, it'll be right 100% of the time!

![Balanced vs. unbalanced dataset](balanced-datasets.svg)

### Shuffled
Finally, every time you train, your data should be shuffled anew.

You want to prevent the network from learning any specific patterns that might be inherent in your training data. These patterns will almost certainly not be present when you go to predict. A common way this can happen is when categories are concatenated one after the other, and the model learns to predict based on where it is in the dataset.

If you always train your dog to sit, lie down, and play dead in the same order, after a while, the dog will begin to anticipate the moves, instead of learning to associate the commands with the actions. She will expect a treat and you will have no one but yourself to blame.

Shuffling your data breaks the ability to learn those associations, and will strengthen the connections that we actually want it to learn.

![Shuffled](shuffled.svg)

## Other datasets

You've got a well balanced training dataset that you're shuffling consistently, and its representative of your real life data. How do you know how well your model is performing on this data? You can measure this by setting aside chunks of your dataset as **test data** and **validation data**.

## How is data stored and processed

To feed data to a neural net, it first needs to be transformed into a numeric form.

This will depend on your datatype. For images, you generally take pixel data and crush it. If its audio, you might take the samples, or maybe take some higher level representation, like a mel spectogram. If its text, you might assign each word a number, or you might use a pretrained set of word embeddings.

Whatever the type of data, the end product is the same: a tensor. What is a tensor?

Tensors are backed up, in Javascript, by Typed Arrays. Typed arrays blah blah.

Memory management, these are hefty buggers. You gotta clear them.

## Manipulating data

We've mostly been discussing data as "things" and their "labels". For instance, an image and its label of dog.

Prior to feeding data, we need to separate our "things" from their "labels", and package up "things" and "labels" into 2 tensors.

Here's how that would look:

---

Tensorflow and other libraries give you convenience methods for shuffling data and slicing into batches automatically. You should take advantage of these. Occasionally you'll run into situations where you need to handle these yourself - for instance, if you have some particularly exotic form of data augmentation (we'll discuss what that is later).

In that case, here's an example function:

Also with labels, you want something called one hots.

## Data Augmentation

This stuff is cool. You supplement your dataset.

# Training

## Examples

Splitting data into buckets

Train, test, validation.

https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7

## Practice

Iteration, hyperparameters (learning rate)

## Rewards

*Dunno what to do about this section.*

Loss

Forward and backward propagation

Weights -> next section

# Models

## The component parts

Neurons

Weights

Layers

Optimizer

## Non Linearity

An oft-repeated claim from traditional machine learning folks is that traditional statistical methods outperform in many cases deep learning. They're also faster and more interpretable.

This is all true. Take this linear shit:

linear

You do linear regression on it and you're done.

Where deep learning shines is with non linear data and where you don't want to do feature engineering.

The reason deep learning is capable of learning non linear data is because of activation functions. Let's look at a few:

Activation Functions

So let's take some sample moons data and see how it does.

## Transfer Learning

What they are

How to find them and understand them

How to build a new model on top of them.





# Not included

gradient descent

Bugs

await next frame

CNNs

Building image classification from scratch
