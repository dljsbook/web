---
layout: page
title: "Training"
date: "2018"
description: "An Overview of the Training Process of a Neural Net"

---

import { ImageClassifier } from '@dljsbook/ui/src/index.tsx';

<ImageClassifier />

I'm a dog person. I spend an inordinate amount of time on Instagram looking at hilarious dogs.

Turns out, training a dog...

I'm currently in the process of adopting a dog, and in preparation I've been looking into dog trainers. If you've never trained a dog before, it's a hell of a good time.

First, the dog has to demonstrate the behavior you're trying to train. For instance, if you're training them to sit - let's call this an **example**. Once the dog has done the behavior, you give the dog some **reward**, like a treat. In this way, the **example** is associated with a **reward**. You then repeat this process, over and over, until the dog gets it. This **repetition** helps cement the link between the behavior and the expected reward.

It turns out that these three things are also used to train a machine. Let's examine each of these and see how they apply to deep learning.

# Examples

Below is an implementation of an image classifier. It receives an input source, which can be either your webcam, or some set of images, and it will predict whether the image belongs to one of two **classes**.

Try capturing some data below.

Isn't that amazing? This wasn't something you could do ten years ago, even five years ago. I think this is amazing.

Let's talk about what's going on here. Here is the data you provided:

[SHOW TABLE OF IMAGES, left and right]

This is known as **training data**. The machine uses this data to train itself on being able to classify an image as category one or category two.

Depending on how you trained your model, you may have gotten high accuracy, or maybe awful accuracy. There's three things to note about good **training data**.

### Representative
One thing that's important is to provide data that is representative of the data you wish to see. For instance, if your training data consists of well-lit data like:

![Categories of data](categories.svg)

yet the data you predict on looks like:

![Prediction](pred.svg)

the machine will struggle to predict successfully.

Commonly with image data, a non-representative dataset will appear when a model is trained only with well-lit, high quality photography, only to receive grainy, low resolution uploaded photos from users. In this case, the model often fails to correctly identify and classify photos. 

Remember, a neural net brings no common sense to the table. It only knows what it sees during training. It'd be like training your dog to "Sit!" on command, then switch from English to Spanish and expect her to understand.

### Well-Balanced
You also want your dataset to be well balanced. You want as close to the same number of data points per category. If one category is overrepresented, you have an **unbalanced dataset**. An unbalanced dataset can lead to problems with prediction. For instance, if 90% of your data belongs to category one, your model might very well learn to assume that everything belongs to category one. After all, 90% of the time, it'll be right 100% of the time!

![Balanced vs. unbalanced dataset](balanced-datasets.svg)

### Shuffled
Finally, every time you train, your data should be shuffled anew.

You want to prevent the network from learning any specific patterns that might be inherent in your training data. These patterns will almost certainly not be present when you go to predict. A common way this can happen is when categories are concatenated one after the other, and the model learns to predict based on where it is in the dataset.

If you always train your dog to sit, lie down, and play dead in the same order, after a while, the dog will begin to anticipate the moves, instead of learning to associate the commands with the actions. She will expect a treat and you will have no one but yourself to blame.

Shuffling your data breaks the ability to learn those associations, and will strengthen the connections that we actually want it to learn.

![Shuffled](shuffled.svg)

## Other datasets

You've got a well balanced training dataset that you're shuffling consistently, and its representative of your real life data. How do you know how well your model is performing on this data? You can measure this by setting aside chunks of your dataset as **test data** and **validation data**.

### Test Data

You test the performance of your model against a holdout set of data commonly referred to as a **test dataset**. (*Different folks use different names for the same underlying concepts, which can make it hard when first learning. Learn the concepts and you'll be able to adapt no matter the language used*).

A **test dataset** is data the machine *has never seen before*. You hold it in reserve, like a savings account. Just like your **training data**, this data consists of both examples and labels, but because this is data the model has never encountered, it serves as a useful barometer for how accurate your model performs. (Remember, this dataset should also be representative - it should not differ substantially from your training dataset).

While its important to get good accuracy on your training dataset, the true test of your model is the score for your **test dataset**. If you can score well against this data, you're golden.

Common practice is to set aside some percentage of your total dataset as your test dataset. A good rule of thumb is:

10%

Let's see how that looks with our image classifier from above.

[IMAGE CLASSIFIER DIVIDED]

Is your dataset balanced? I can't tell - I'm just a book - but look to see whether the test data is representative of your training data.

### Validation Data

There's a third type of data you might encounter, and that's called **validation data**. This is some portion of your dataset that is held back during training, and used to measure the accuracy during training. This is not strictly mandatory (most of these are just rules of thumb, and can and do change based on your use case).

There are two techniques here, you can either, for each epoch (what is an epoch?) set aside some portion of the training data as your validation data; or you can set aside the same portion each time. Visual demonstrations of both techniques are below.

If you have enough data - and how much is enough? - setting aside a dedicated validation portion can be a good strategy, since it gives you confidence that the accuracy is based on the model learning independently of the validation data. If you're dealing with smaller datasets, and in the browser we often are, data is your most precious resource and you want to be strategic in how you deploy it. In these cases, setting aside a small portion each cycle can make more sense. In this case, make sure
to slice this off after you shuffle, so you're always testing on a different portion.


So you can begin to see what's going on. You give the model a tensor - an array - of input values, along with a tensor of expected labels. In a perfect world, the values the network predicts match up with the labels. The difference with this reality - how wrong the model is - is the thing we wish to work on.

How does that process work? We'll learn more about that in the next two sections.

# Practice

You wouldn't expect your dog to learn a trick on the first try; hell, sometimes not even on the hundredth try. Similarly, a neural network needs to work through many attempts before it's able to hone on correct answers.

The training process is an **iterative** one. It takes multiple cycles of training to get better. The more data, and the more complex the data, the more cycles of training you will need. The more complex the model, the more cycles of training you need. The more accurate you want your model to be, the more cycles of training you will need (up to a point - too many cycles can actually
lead to overfitting which we will touch on later).

Let's see an example. Here's a simple line with a slope of 2.

[LINE] - maybe you can modify slope and intercept?

If I ask you to look at this line and predict the value of **Y** if **X** is 5, you could figure it the answer was 10 just by looking at the line. Our neural net cannot do this. Remember, it has no inherent intelligence - no ability to deduce using reasoning and logic. The only way it can learn is iteratively, through repeated training on data that we feed it.

With this, you can hover over the graph to see what the model predicts a value at. Click to leave a point on the graph.

[GRAPH]

In its default state, the network makes random predictions. That's because the component parts of the neural network are initialized randomly. The component parts in this case are the "weights". What are weights?

Weights make up a large part of what neural nets do. Here's a live diagram:

[NN]

Change weights. See what that does to different input values. These weights* are the things that the neural network changes as it trains. *There are also biases.

Let's go back to our line. It's pretrained. You can slide the slider to see the predictions and how they change over time.

[EXAMPLE]

What is an epoch?

You can also see the weights of the model changing over time:

[NN with slider]

One thing to emphasize here is that the network gets better, iteravitely, over time. It probably will never be perfect. This is an important mental shift from traditional software engineering. You have to become comfortable with a realm of acceptability.

In general, the more cycles of training you can afford, the better the model will be. There are limits to this, like overfitting, which we will discuss later. One way of increasing the number of cycles is by using "minibatches", where you take chunks of your data. Often times, this is a necessity borne out of memory - especially on servers, where its infeasible to train on gigabytes at a time, but especially in the browser, where adding big chunks of data into memory can block the UI.

Speaking of blocking the UI, tfjs provides two important memory management methods:

tf.tidy
tf.awaitFrame

use them.

# Rewards


* here, dig into loss, gradient descent, and other goodies
    * explain how forward propagation works. how does a network wind up with a prediction?
        * expanding tensor dimensions, and concatenating
        * number of classes
    * talk about how loss is the sum of the differences
        * talk about how you want to minimize loss, and how you do that with gradient descent
        * talk about what kinds of losses there are

[Comic of dog with human saying a bunch of nonsense, and dog listening for the food parts]

Rewards are what makes training possible. For dogs, its food. For neural networks, its something called **loss**.

Loss is a measure of how wrong a network's predictions are. The network is incentivized to reduce loss as much as possible; that's its reward. (Pretty meager, I know; who am I to judge what motivates a computer!)

Let's go back to our line example, and see our loss for epoch 0:

[SHOW NUMBERS]

Then, let's see it at epoch 1:

[SHOW NUMBERS]

As is evident, the loss from epoch 1 is lower than epoch 0. The network is "learning".

Here's some formal terms for ya: **forward propagation** is sending numbers through the network, and **backwards propagation** is basically sending data backwards through the network. This backwards step is how the network trains.

Let's see a simple example. We start with totally randomized weights. We then send some numbers through the network. We get our loss. We then adjust the weights some large amount, TIMES what's called the learning rate. Learning rate represents a **hyperparameter** - an argument you can tune in the network to get better results. There's no right answer for what this should be, but there are guidelines.

Why don't we want to just take a big leap? Iteration is the key. You could overshoot. Let's see an example - if we had a huge learning rate. In fact, you can play with the learning rate.


---

## Code

Let's look at some code. Here's our training function:

```
import model from '@dljsbook/models/image_classification';
model.fit(training_data, training_labels, {
  callbacks: 
});
```

