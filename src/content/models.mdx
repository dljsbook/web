---
layout: page
title: "Models"
date: "2018"
description: "On the catwalk yeah, on the catwalk"

---

## The component parts

Neurons

Weights

Layers

Optimizer

## Non Linearity

An oft-repeated claim from traditional machine learning folks is that traditional statistical methods outperform in many cases deep learning. They're also faster and more interpretable.

This is all true. Take this linear shit:

linear

You do linear regression on it and you're done.

Where deep learning shines is with non linear data and where you don't want to do feature engineering.

The reason deep learning is capable of learning non linear data is because of activation functions. Let's look at a few:

Activation Functions

So let's take some sample moons data and see how it does.

## Transfer Learning

What they are

How to find them and understand them

How to build a new model on top of them.







---

Transfer learning

[AT SOME POINT IN CNNs: TALK ABOUT MNIST]

Talk about convolutional networks

Talk about activation functions, and how networks are put together. Revisit training and discuss forward and back propagation.
