---
layout: page
title: "Data"
date: "2018"
description: "Oh, the data you'll meet"

---



All neural nets begin with data.

At its core, a neural network is simply a set of mathematical functions; theory drawn in code. Neural nets only come alive in the presence of data.

What is data? Data is the stuff you measure, collect, manipulate, and ultimately make predictions upon.

Data can be anything capable of being represented in numeric form in a computer. That includes a whole lot. Obviously numbers, like that found in spreadsheets or database tables, can be data.

[SPREADSHEET DATA]

Images are another common source of data:

[IMAGE DATA]

Audio and video are also commonly seen:

[AUDIO and VIDEO data]

Text, PDFs, documents, webpages - anything with text is data:

[TEXT data]

Laser recordings from the tops of autonomous cars, EMRs from medical devices, even EKGs from brain waves are data. If it can be represented in numerical form - and really, anything we can measure is capable of a numeric representation - then it can be data.

The particular nature of your data dictates everything about the Neural Network that you build.

## The Platonic Dataset

Mostly we discuss data in aggregate, as a dataset.

What defines a good dataset? It depends.

More data is usually better. But if you're working on a well understood or well solved problem, you can get by with less. If the problem is novel, you'll need more. If you're working with some exotic data form, you'll probably need a lot more.

Regardless of the size and composition of your dataset, you should take care of three considerations when compiling your dataset: it should be **representative**, it should be **well-balanced**, and it should be **shuffled**.

### Representative
It is important that the dataset you use to train your network be *as similar as possible* to the data the network will be predicting upon. That is, that it be **representative** of the eventual dataset used for prediction. If your training data looks like:

![Categories of data](categories.svg)

But the data it predicts on is more like:

![Prediction](pred.svg)

The network will struggle to predict successfully.

A common example of a non-representative dataset in image recognition is when a network is trained only with well-lit, high quality photography, yet when it is deployed in production, receives grainy, low resolution uploaded photos from users.

This is like training your dog to "Sit!" and then switching from English to Spanish, expecting her to understand.

A neural network brings no inherent common sense to the table. The only things it learns are from the examples you provide it. Therefore, strive to provide as representative a sample of data as possible.

> Ruler detector  AI trained to classify skin lesions as potentially cancerous learns that lesions photographed next to a ruler are more likely to be malignant.  Andre Esteva et al, 2017  Dermatologist-level classification of skin cancer with deep neural networks https://www.nature.com/articles/nature21056.epdf 

### Well-Balanced
A **well-balanced** dataset means that all categories in the dataset are of roughly equal number.

If one category is overrepresented in your data, you have an **unbalanced dataset**.

![Balanced vs. unbalanced dataset](balanced-datasets.svg)

An unbalanced dataset can lead to problems with prediction. For instance, assume 90% of your data belongs to category one, and 10% to category two. Rather than learning the specific traits of your data, your model might very well learn to assume that a better strategy is to predict everything as belonging to category one. This will net it a 90% success score!

What do you do if one category inherently has more data?

There are a few strategies. You can oversample from the smaller category - in essence, just copy and paste data randomly. If you have enough data, you can reduce the size of the overabundance category. Finally, you can use data augmentation techniques to create new data and seek to balance the categories that way. We'll discuss data augmentation later.

### Shuffled

Every time you train your model, you should take care that your data is **shuffled**, meaning your data is sorted randomly each training cycle.

*(be careful to use the same sorting order for both your labels and your features).*

You want to prevent the network from learning specific patterns that might be inherent in your training data. These patterns will almost certainly not be present when you go to predict, and will stump your network in production.

![Shuffled](shuffled.svg)

If you always train your dog to sit, lie down, and play dead in the same order, the dog will start to anticipate the pattern of moves, instead of learning to associate each command with a specific action.

Shuffling your data breaks the ability to learn those associations, and will strengthen the connections that we actually want the network to learn.

In practice, a common way this can occur is if your categories are concatenated one after another. The model learns to predict the category based on which categories have come before. This precise scenario occurred in an image classification study by Ellefsen et all, where neural nets learned to classify edible or poisonous mushrooms that were presented in alternated order, failing to actually recognize any features of the images themselves.

> Data order patterns | Neural nets evolved to classify edible and poisonous mushrooms took advantage of the data being presented in alternating order, and didn't actually learn any features of the input images | Ellefsen et al, 2015 | Neural modularity helps organisms evolve to learn new skills without forgetting old skills | http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004128 | 

> https://kottke.org/18/11/how-ai-agents-cheat

## Storing and Processing Data

Neural networks operate on numbers, so any data you provide must first be transformed into numeric form.

There are a number of common strategies depending on the type of data you're using. For images, pixel data is already in numeric form (across three RGB channels), though the pixel values are generally resampled to be between -1 and 1. Audio files might be processed into some higher level representation, like a mel spectogram. Words can each be assigned a number, or better, a set of word embeddings provided by a pretrained corpus.

Most Deep Learning frameworks expect data in the form of tensors. Tensors are a concept imported from the world of mathematics, and provide a container for storing multidimensional arrays.

[IMAGE OF TENSOR]

Tensors contain data, but they also have dimension, shape, type, and describe valid transformations. In particular, the dimension - also known as a tensor's rank - is important to understand. A tensor's shape is tied to that and is also important. Here's a [3x3x3] tensor:

[IMAGE OF TENSOR]

You can create a tensor with:

```
tf.tensor1d([1, 2]);
```

Tensors are backed up, in Javascript, by `TypedArray`s.

`TypedArray`s provide an efficient mechanism for handling data. 

https://thekevinscott.com/tensors-in-javascript/

Tensors can do a number on your browser if you leave them sitting around, so you need to get comfortable with doing some garbage collection. `Tensorflow.js` provides two mechanisms for handling unneeded tensors.

First, you can manually dispose of a tensor by calling `tensor.dispose()`.

An easier option is to call the function `tf.tidy`:

```
tf.tidy(() => {
  const x = tf.tensor1d();
});

// x will be cleared, maybe?
```

`tf.tidy` only accepts synchronous functions, so anything being returned from within a promise or async function is no good.

## Manipulating data

We've mostly been discussing data as "things" and their "labels". For instance, an image and its label.

[Image and a label]

We've been talking about our **training data** as a monolithic collection. In fact, we will consistently separate our data into "features" and "labels". Our features are the images themselves, and the labels describe which category they should be. You need these to be in the same order - matched up - but you also need them to be discrete tensors.

Prior to feeding data, we need to separate our "things" from their "labels", and package up "things" and "labels" into 2 tensors.

Here's how that would look:

[IMAGE OF SEPARATED]

We'll talk about training in the next section, but you'll often want to slice and rearrange your data. Tensorflow.js and other libraries give you convenience methods for shuffling data and slicing into batches automatically. You should take advantage of these. Occasionally you'll run into situations where you need to handle these yourself - for instance, if you have some particularly exotic form of data augmentation (we'll discuss what that is later).

In that case, here's an example function:

```
return sliced data
```

All neural nets deal in numbers. Whether your training data consists of images, audio, text or some other exotic data format, you will need to convert it into numbers for consumption by the machine. The particular method to do this will depend on your data and may often be bespoke to your particular use case. Labels are the same - you will need to take your string labels - "category one " and "category two" - and turn these into numbers.

Your numbers, for images, might be the pixel data. For audio, the samples, or maybe a Fourier transform. For text, it might be numeric encodings for each word.

Your labels also need to be numbers, and we often want to perform something called "one hot" on them. *Where does the name "one hot" come from?*

Next, you'll need to convert your labels into numeric form. However, it's not as simple as assigning a number to each category. To demonstrate, let's say you're classifying three categories of fruit:

raspberry - 0
blueberry - 1
strawberry - 2
Denoting numbers like this can imply a relationship where one does not exist, since these numbers are considered ordinal values; they imply some order in the data. Real world consequences of this might be that the network decides that a blueberry is something that is halfway between a raspberry and a strawberry, or that a strawberry is the "best" of the berries.

Strawberry Inception

To prevent these incorrect assumptions we use a process called "one hot encoding", resulting in data that looks like:

raspberry  - [1, 0, 0]
blueberry  - [0, 1, 0]
strawberry - [0, 0, 1]
(Two great articles that go into more depth on one hot encoding are here and here.) We can leverage Tensorflow.js's built in oneHot functions to translate our labels:

function oneHot(labelIndex, classLength) {
  return tf.tidy(() => tf.oneHot(tf.tensor1d([labelIndex]).toInt(), classLength));
};
This function takes a particular number (labelIndex, a number that corresponds to a label) and translates it to a one hot encoding, given some number of classes (classLength). We can use the function with the following bit of code, that first builds a mapping of numbers-to-labels off the incoming array of labels, and then builds a Tensor containing those one-hot encoded labels:


## Data Augmentation

Neural networks thrive on data, yet sometimes it is a challenge to find enough data for your models. In these cases you can **augment** your data to expand your dataset.

Augmentation is simply transforming your data in such a way that it provides alternative versions of the same data, while still falling within the parameters of what the neural network expects. For example, if you're training a network to recognize animals, you might take an image of a horse:

[HORSE]

And try various rotations, zooms, and flipping horizontally:

[AUGMENTATIONS]

On the other hand, you probably wouldn't want to do this:

[VERTICAL AUGMENTATION]

You probably won't be seeing many upside down horses (unless your users are out to screw you, in which case you might!)

But it does depend on your use case. For instance, if you're trying to recognize topological features from satellite imagery, both vertical and horizontal flipping *would* make sense.

This is one area I think it pays to bring your creativity to bear. For instance, in a project I'm working on, I'm trying to recognize laughter in television shows. Mixing tracks of laughter in with random talkings produces a lot of synthetic data.

One thing to keep in mind when working with augmentations is to build your pipeline so that you're not augmenting your validation and test data (unless you intend to transform your predictive data as well - dunno why'd you do that). This can lead to some complicated use cases which are up to you to solve. Good luck buddy.
