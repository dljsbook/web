---
layout: page
title: "Data"
date: "2018"
description: "Oh, the data you'll meet"

---

All neural nets begin with data, on both a conceptual level and a physical level.

A neural net, at its core, is just a set of mathematical functions - theory represented in code. Neural nets only come "alive" in the presence of data.

What is data? Data are the things you measure, the things you make predictions on.

Data can be anything representable in numeric form in a computer. It can be numbers, or tabular data. It can be images, audio, or video. It can be text, PDFs, documents, webpages. It can be laser recordings from the top of autonomous cars, EMRs from medical devices, even EKGs from brain waves. If it can be represented in numerical form - and really, anything we can measure is capable of becoming numeric - then it can be data.

The particular nature of your data will dictate everything about the Neural Network that you build.

## The Platonic Dataset

What defines a good dataset? Your dataset should be as close as possible to your expected data. In particular, you should keep in mind three things. A dataset should be:

* Representative
* Well-balanced
* Shuffled

Let's examine each of these.

### Representative
One thing that's important is to provide data that is representative of the data you wish to see. For instance, if your training data consists of well-lit data like:

![Categories of data](categories.svg)

yet the data you predict on looks like:

![Prediction](pred.svg)

the machine will struggle to predict successfully.

Commonly with image data, a non-representative dataset will appear when a model is trained only with well-lit, high quality photography, only to receive grainy, low resolution uploaded photos from users. In this case, the model often fails to correctly identify and classify photos. 

Remember, a neural net brings no common sense to the table. It only knows what it sees during training. It'd be like training your dog to "Sit!" on command, then switch from English to Spanish and expect her to understand.

### Well-Balanced
You also want your dataset to be well balanced. You want as close to the same number of data points per category. If one category is overrepresented, you have an **unbalanced dataset**. An unbalanced dataset can lead to problems with prediction. For instance, if 90% of your data belongs to category one, your model might very well learn to assume that everything belongs to category one. After all, 90% of the time, it'll be right 100% of the time!

![Balanced vs. unbalanced dataset](balanced-datasets.svg)

### Shuffled
Finally, every time you train, your data should be shuffled anew.

You want to prevent the network from learning any specific patterns that might be inherent in your training data. These patterns will almost certainly not be present when you go to predict. A common way this can happen is when categories are concatenated one after the other, and the model learns to predict based on where it is in the dataset.

If you always train your dog to sit, lie down, and play dead in the same order, after a while, the dog will begin to anticipate the moves, instead of learning to associate the commands with the actions. She will expect a treat and you will have no one but yourself to blame.

Shuffling your data breaks the ability to learn those associations, and will strengthen the connections that we actually want it to learn.

![Shuffled](shuffled.svg)


## How is data stored and processed

Neural networks operate on numbers, so any data you provide must first be transformed into numeric form.

There are a number of common strategies depending on the type of data you're using. For images, pixel data is already in numeric form (across three RGB channels), though the pixel values are generally resampled to be between -1 and 1. Audio files might be processed into some higher level representation, like a mel spectogram. Words can each be assigned a number, or better, a set of word embeddings provided by a pretrained corpus.

Most Deep Learning frameworks expect data in the form of tensors. Tensors are a concept imported from the world of mathematics, and provide a container for storing multidimensional arrays.

[IMAGE OF TENSOR]

Tensors contain data, but they also have dimension, shape, type, and describe valid transformations. In particular, the dimension - also known as a tensor's rank - is important to understand. A tensor's shape is tied to that and is also important. Here's a [3x3x3] tensor:

[IMAGE OF TENSOR]

Tensors are backed up, in Javascript, by `TypedArray`s.

`TypedArray`s provide an efficient mechanism for handling data. 

https://thekevinscott.com/tensors-in-javascript/

Tensors can do a number on your browser if you leave them sitting around, so you need to get comfortable with doing some garbage collection. `Tensorflow.js` provides two mechanisms for handling unneeded tensors.

First, you can manually dispose of a tensor by calling `tensor.dispose()`.

An easier option is to call the function `tf.tidy`:

```
tf.tidy(() => {
  const x = tf.tensor1d();
});

// x will be cleared, maybe?
```

`tf.tidy` only accepts synchronous functions, so anything being returned from within a promise or async function is no good.

## Manipulating data

We've mostly been discussing data as "things" and their "labels". For instance, an image and its label.

[Image and a label]

We've been talking about our **training data** as a monolithic collection. In fact, we will consistently separate our data into "features" and "labels". Our features are the images themselves, and the labels describe which category they should be. You need these to be in the same order - matched up - but you also need them to be discrete tensors.

Prior to feeding data, we need to separate our "things" from their "labels", and package up "things" and "labels" into 2 tensors.

Here's how that would look:

[IMAGE OF SEPARATED]

We'll talk about training in the next section, but you'll often want to slice and rearrange your data. Tensorflow.js and other libraries give you convenience methods for shuffling data and slicing into batches automatically. You should take advantage of these. Occasionally you'll run into situations where you need to handle these yourself - for instance, if you have some particularly exotic form of data augmentation (we'll discuss what that is later).

In that case, here's an example function:

```
return sliced data
```

All neural nets deal in numbers. Whether your training data consists of images, audio, text or some other exotic data format, you will need to convert it into numbers for consumption by the machine. The particular method to do this will depend on your data and may often be bespoke to your particular use case. Labels are the same - you will need to take your string labels - "category one " and "category two" - and turn these into numbers.

Your numbers, for images, might be the pixel data. For audio, the samples, or maybe a Fourier transform. For text, it might be numeric encodings for each word.

Your labels also need to be numbers, and we often want to perform something called "one hot" on them. *Where does the name "one hot" come from?*

Next, you'll need to convert your labels into numeric form. However, it's not as simple as assigning a number to each category. To demonstrate, let's say you're classifying three categories of fruit:

raspberry - 0
blueberry - 1
strawberry - 2
Denoting numbers like this can imply a relationship where one does not exist, since these numbers are considered ordinal values; they imply some order in the data. Real world consequences of this might be that the network decides that a blueberry is something that is halfway between a raspberry and a strawberry, or that a strawberry is the "best" of the berries.

Strawberry Inception

To prevent these incorrect assumptions we use a process called "one hot encoding", resulting in data that looks like:

raspberry  - [1, 0, 0]
blueberry  - [0, 1, 0]
strawberry - [0, 0, 1]
(Two great articles that go into more depth on one hot encoding are here and here.) We can leverage Tensorflow.js's built in oneHot functions to translate our labels:

function oneHot(labelIndex, classLength) {
  return tf.tidy(() => tf.oneHot(tf.tensor1d([labelIndex]).toInt(), classLength));
};
This function takes a particular number (labelIndex, a number that corresponds to a label) and translates it to a one hot encoding, given some number of classes (classLength). We can use the function with the following bit of code, that first builds a mapping of numbers-to-labels off the incoming array of labels, and then builds a Tensor containing those one-hot encoded labels:


## Data Augmentation

Neural networks thrive on data, yet sometimes it is a challenge to find enough data for your models. In these cases you can **augment** your data to expand your dataset.

Augmentation is simply transforming your data in such a way that it provides alternative versions of the same data, while still falling within the parameters of what the neural network expects. For example, if you're training a network to recognize animals, you might take an image of a horse:

[HORSE]

And try various rotations, zooms, and flipping horizontally:

[AUGMENTATIONS]

On the other hand, you probably wouldn't want to do this:

[VERTICAL AUGMENTATION]

You probably won't be seeing many upside down horses (unless your users are out to screw you, in which case you might!)

But it does depend on your use case. For instance, if you're trying to recognize topological features from satellite imagery, both vertical and horizontal flipping *would* make sense.

This is one area I think it pays to bring your creativity to bear. For instance, in a project I'm working on, I'm trying to recognize laughter in television shows. Mixing tracks of laughter in with random talkings produces a lot of synthetic data.

One thing to keep in mind when working with augmentations is to build your pipeline so that you're not augmenting your validation and test data (unless you intend to transform your predictive data as well - dunno why'd you do that). This can lead to some complicated use cases which are up to you to solve. Good luck buddy.
