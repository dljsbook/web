---
layout: page
title: "Training"
date: "2018"
description: "An Overview of the Training Process of a Neural Net"

---

Training refers to sending data through the neural network in an effort to tune its weights to make better predictions.

The process has some surface level similarity to training a dog, in three ways.

When training a dog, first, the dog has to demonstrate the behavior you're trying to train. For instance, if you're training them to sit - let's call this an **example**. Once the dog has done the behavior, you give the dog some **reward**, like a treat. In this way, the **example** is associated with a **reward**. You then repeat this process, over and over, until the dog gets it. This **repetition** helps cement the link between the behavior and the expected reward.

It turns out that these three things are also used to train a machine. Let's examine each of these and see how they apply to deep learning.

## Examples

When you train a dog, you need to provide examples of the behavior you wish to reward, and teach the dog to associate each example with a command.

Similarly, with a neural network, you compile a list of examples that demonstrate what you wish the network to learn, and you teach the network to associate each example with a label.

In the previous chapter, we discussed what makes up a good dataset, namely that it be representative, balanced, and shuffled. Your dataset includes the examples that you feed to the network for training, along with the labels for each datum.

How do you know that your network is, say, learning the features of your data, and not something you don't want it to?

You measure that by setting aside chunks of your dataset as **test data** and **validation data**. These separate buckets provide you an unbiased sample by which to measure your network.

https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7

### Training Data

This bucket comprises the bulk of your dataset. This is the data your model consumes and learns from.

Assuming you have no need to measure the performance of your network, this bucket will comprise the entirity of your dataset.

### Test Data

You test the performance of your model against a holdout set of data commonly referred to as a **test dataset**. (*Different folks use different names for the same underlying concepts, which can make it hard when first learning. Learn the concepts and you'll be able to adapt no matter the language used*).

A **test dataset** is data the machine *has never seen before*. You hold it in reserve, like a savings account. Just like your **training data**, this data consists of both examples and labels, but because this is data the model has never encountered, it serves as a useful barometer for how accurate your model performs. (Remember, this dataset should also be representative - it should not differ substantially from your training dataset).

While its important to get good accuracy on your training dataset, the true test of your model is the score for your **test dataset**. If you can score well against this data, you're golden.

*Is your dataset balanced? I can't tell - I'm just a book - but look to see whether the test data is representative of your training data.*

### Validation Data

There's a third bucket you might separate, a **validation set**. This is similar to the test set from above, in that its some portion of your dataset that is held back during training, and used to measure the accuracy *during* training.

Validation data can either be a set slab of data separated before training begins, or it can be randomly selected at the start of each epoch. The former provides a more accurate ready of the model's ongoing performance, but can rob you of precious data if you don't have much.

Validation data especially helps with tuning hyperparameters.

If you have enough data, setting aside a dedicated validation portion can be a good strategy, since it gives you confidence that the accuracy is based on the model learning independently of the validation data. If you're dealing with smaller datasets, and in the browser we often are, data is your most precious resource and you want to be strategic in how you deploy it. In these cases, setting aside a small portion each cycle can make more sense. In this case, make sure
to slice this off after you shuffle, so you're always testing on a different portion.

How much data should make up each dataset? It depends. For datasets upwards of 100,000 data points, 10,000 points each for test and validation sets is more than enough and you shouldn't exceed 10,000. For smaller datasets, it depends on how big your dataset is, and how complex your model is. A very rough starting point is 80% for training, and 10% each for validation and test, but you'll have to experiment with this.

Anyways - we're now giving the model a tensor of input values, and a tensor of labels. We wish for our predicted values to match up perfectly with the provided labels. The difference with this reality - how wrong the model is - is the parameter we wish to improve.

## Practice

Iteration, hyperparameters (learning rate)

You wouldn't expect your dog to learn a trick on the first try. Similarly, a neural network needs to work through many attempts before it's able to land on the correct answers.

The training process is an **iterative** process. It takes multiple cycles of training to get better. The more data, and the more complex the data, the more cycles of training you will need. The more complex the model, the more cycles of training you need. The more accurate you want your model to be, the more cycles of training you will need (up to a point - too many cycles can actually lead to overfitting which we will touch on later).

Let's see an example. Here's a simple line with a slope of 2.

[LINE]

If I ask you to look at this line and predict the value of **Y** if **X** is 5, you could figure it the answer was 10 just by looking at it. Our neural network cannot do this. Remember, it has no inherent intelligence - no ability to use reasoning or logic. The only way it learns is iteratively, through repeated training on data that we feed it.

With this, you can hover over the graph to see what the model predicts a value at. Click to leave a point on the graph.

[GRAPH]

In its default state, the network makes random predictions. That's because the component parts of the neural network are initialized randomly.

*The component parts in this case are the "weights". What are weights? Weights make up a large part of what neural nets do. Here's a live diagram: Change weights. See what that does to different input values. These weights are the things that the neural network changes as it trains. There are also biases.*

Let's go back to our line. It's pretrained. You can slide the slider to see the predictions and how they change over time.

[EXAMPLE]

What is an epoch?

You can also see the weights of the model changing over time:

[NN with slider]

One thing to emphasize here is that the network gets better, iteravitely, over time. It probably will never be perfect. This is an important mental shift from traditional software engineering. You have to become comfortable with a realm of acceptability.

In general, the more cycles of training you can afford, the better the model will be. There are limits to this, like overfitting, which we will discuss later. One way of increasing the number of cycles is by using "minibatches", where you take chunks of your data. Often times, this is a necessity borne out of memory - especially on servers, where its infeasible to train on gigabytes at a time, but especially in the browser, where adding big chunks of data into memory can block the UI.

Speaking of blocking the UI, tfjs provides two important memory management methods:

tf.awaitFrame

use them.

*Why don't we want to just take a big leap? Iteration is the key. You could overshoot. Let's see an example - if we had a huge learning rate. In fact, you can play with the learning rate.*

## Rewards

Rewards are what makes training possible. For dogs, its usually food. For neural networks, its a thing called **loss**.

Loss is a measure of how wrong a network's predictions are. The network is incentivized to reduce loss as much as possible. That's its reward.

Let's go back to our line example, and see our loss for epoch 0:

[SHOW NUMBERS]

Then, let's see it at epoch 1:

[SHOW NUMBERS]

As is evident, the loss from epoch 1 is lower than epoch 0. The network is "learning".

Loss, formally, is the sum of the difference of predicted outcomes from actual outcomes. So there's no sort of set scale to it, but in almost all cases you want it to trend towards 0 over time.

*Here's some formal terms for ya: **forward propagation** is sending numbers through the network, and **backwards propagation** is basically sending data backwards through the network. This backwards step is how the network trains.*

Let's see a simple example. We start with totally randomized weights. We then send some numbers through the network. We get our loss. We then adjust the weights some large amount, TIMES what's called the learning rate. Learning rate represents a **hyperparameter** - an argument you can tune in the network to get better results. There's no right answer for what this should be, but there are guidelines.

So we're tweaking the weights. What are weights? Well, that leads us into our next section: the architecture of your model.

https://kottke.org/18/04/the-lebowski-theorem-of-machine-superintelligence




[Comic of dog with human saying a bunch of nonsense, and dog listening for the food parts]





---

## Code

Let's look at some code. Here's our training function:

```
import model from '@dljsbook/models/image_classification';
model.fit(training_data, training_labels, {
  callbacks: 
});
```

